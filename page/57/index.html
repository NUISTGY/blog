<!DOCTYPE html><html lang="zh-Hans"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Do what you want to do !"><meta name="keywords" content><meta name="author" content="GeYu"><meta name="copyright" content="GeYu"><title>Do not go gentle into that good night ~ | Yu's Blog</title><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="https://images5.alphacoders.com/423/423529.jpg"></div><div class="author-info__name text-center">GeYu</div><div class="author-info__description text-center">Do what you want to do !</div><div class="follow-button"><a href="https://github.com/NUISTGY">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">228</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">83</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">46</span></a></div></div></div><nav id="nav" style="background-image: url(https://pic.syst.eu.org/WechatIMG8673.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Yu's Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span></div><div id="site-info"><div id="site-title">Yu's Blog</div><div id="site-sub-title">Do not go gentle into that good night ~</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2021/10/07/PSHRNet/">✨Deep-High-Resolution Representation Learning for Cross-Resolution Person Re-identification</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-10-07</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/行人重识别/">行人重识别</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/行人重识别/">行人重识别</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/论文/">论文</a></span><div class="content"><script src="/assets/js/APlayer.min.js"> </script><div align="center">

<p><a href="https://ieeexplore.ieee.org/document/9591273/authors#citations" target="_blank" rel="noopener"><strong><front size="5">Journal of IEEE TIP (SCI-Q1 Top)</front></strong></a></p>
<center>
G. Zhang, 👉Y. Ge👈, Z. Dong, H. Wang, Y. Zheng and S. Chen
</center>

<p><img src="https://img.gejiba.com/images/de85252fec31e625fecf220fa7b686b8.png" alt></p>
</div>

<h2 id="Contents-🗒"><a href="#Contents-🗒" class="headerlink" title="Contents 🗒"></a><a id="contents-">Contents 🗒</a></h2><ul>
<li><a href="#contents-"><a id="contents-">Contents 🗒</a></a></li>
<li><a href="#introduction-"><a id="introduction-">Introduction 🗒</a></a></li>
<li><a href="#usage-"><a id="usage-">Usage 🔧</a></a></li>
<li><a href="#results-"><a id="result-">Results 🏆</a></a></li>
<li><a href="#acknowledgements-"><a id="acknowledgements-">Acknowledgements 👍</a></a></li>
</ul>
<h2 id="Introduction-🗒"><a href="#Introduction-🗒" class="headerlink" title="Introduction 🗒"></a><a id="introduction-">Introduction 🗒</a></h2><p>We propose a Deep High-Resolution Pseudo-Siamese Framework (PS-HRNet) to solve the cross-resolution person re-ID problem. Specifically, in order to restore the resolution of low-resolution images and make reasonable use of different channel information of feature maps, we introduce and innovate VDSR module with channel attention (CA) mechanism, named as VDSR-CA. Then we reform the HRNet by designing a novel representation head to extract discriminating features, named as HRNet-ReID. In addition, a pseudo-siamese framework is constructed to reduce the difference of feature distributions between low-resolution images and high-resolution images. The experimental results on five cross-resolution person datasets verify the effectiveness of our proposed approach. Compared with the state-of-the-art methods, our proposed PS-HRNet improves 3.4%, 6.2%, 2.5%,1.1% and 4.2% at Rank-1 on MLR-Market-1501, MLR-CUHK03, MLR-VIPeR, MLR-DukeMTMC-reID, and CAVIAR datasets, respectively.</p>
<h2 id="Usage-🔧"><a href="#Usage-🔧" class="headerlink" title="Usage 🔧"></a><a id="usage-">Usage 🔧</a></h2><p>We use apex (A PyTorch Extension) a Pytorch extension with NVIDIA-maintained utilities to streamline mixed precision and distributed training. Some of the code here will be included in upstream Pytorch eventually. The intention of Apex is to make up-to-date utilities available to users as quickly as possible.Installation instructions can be found here: <a href="https://github.com/NVIDIA/apex#quick-start" target="_blank" rel="noopener">https://github.com/NVIDIA/apex#quick-start</a>.</p>
<p>We display the process of the algorithm as an ipynb file, you can use jupyter notebook to view and run it.</p>
<p>You may need HRNet-W32-C ImageNet pretrained models or learn more about HRNet: <a href="https://github.com/HRNet/HRNet-Image-Classification.git" target="_blank" rel="noopener">https://github.com/HRNet/HRNet-Image-Classification.git</a>.</p>
<p>Wanna know more detail of the first phase？ Check this：<a href="https://github.com/NUISTGY/Person-re-identification-based-on-HRNet" target="_blank" rel="noopener">https://github.com/NUISTGY/Person-re-identification-based-on-HRNet</a></p>
<h2 id="Results-🏆"><a href="#Results-🏆" class="headerlink" title="Results 🏆"></a><a id="result-">Results 🏆</a></h2><div align="center">

<p><img src="https://img.gejiba.com/images/6670ce1bd1696c28e0fedd4fbefd676f.png" alt></p>
</div>

<h2 id="Acknowledgements-👍"><a href="#Acknowledgements-👍" class="headerlink" title="Acknowledgements 👍"></a><a id="acknowledgements-">Acknowledgements 👍</a></h2><ul>
<li>This code is built on <a href="https://github.com/HRNet/HRNet-Image-Classification" target="_blank" rel="noopener">HRNet-Image-Classification</a> and <a href="https://github.com/layumi/Person_reID_baseline_pytorch" target="_blank" rel="noopener">Person_reID_baseline_pytorch</a>. We thank the authors for sharing their codes. To the great spirit of open source!</li>
<li>Thank <a href="https://github.com/dzc2000" target="_blank" rel="noopener">Z.Dong</a> and <a href="https://github.com/Rockdow" target="_blank" rel="noopener">H.Wang</a>, they are the most important contributors to the related work of the experiment. If you have any questions in the process of testing, you can send them by email or pose issues.</li>
<li>Thanks for the right to use the GPU workstation provided by Nanyang Technological University.</li>
</ul>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/07/15/论文笔记(一)/">Deep Low-Resolution Person Re-Identification阅读笔记</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-07-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/论文笔记/">论文笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/行人重识别/">行人重识别</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/论文/">论文</a></span><div class="content"><script src="/assets/js/APlayer.min.js"> </script><h1 id="标题及作者"><a href="#标题及作者" class="headerlink" title="标题及作者"></a>标题及作者</h1><p><img src="https://s3.bmp.ovh/imgs/2022/09/05/aa8ff46d06e6cc07.png" alt></p>
<h2 id="方法概况及总结"><a href="#方法概况及总结" class="headerlink" title="方法概况及总结"></a>方法概况及总结</h2><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p><img src="https://s3.bmp.ovh/imgs/2022/09/05/d2fb70f26d148a01.png" alt="整体架构"><br>SING CNN由SR子网(d)和Re-ID子网(e)两部分组成，在模型训练中部署三个流，分别以LR图像(a)、合成LR图像(b)和HR图像(c)作为输入。中间流(b)作为连接图像SR (d)和人Re-ID (e)学习任务的桥梁。</p>
<h3 id="损失函数构建"><a href="#损失函数构建" class="headerlink" title="损失函数构建"></a>损失函数构建</h3><p>损失函数由<strong>SR损失</strong>和<strong>Re-ID</strong>损失联合构建，公式如下：<br><img src="https://s3.bmp.ovh/imgs/2022/09/05/2dfc3dd0e6857260.png" alt="SR损失函数 "><br><img src="https://s3.bmp.ovh/imgs/2022/09/05/f440a4392562d931.png" alt="Re-ID损失函数"><br>L-reid式中：<br><img src="https://s3.bmp.ovh/imgs/2022/09/05/aed7d2417f060ddb.png" alt=" "><br>最后构建联合损失函数如下：<br><img src="https://s3.bmp.ovh/imgs/2022/09/05/194e32f193a83d27.png" alt="联合损失函数"></p>
<h3 id="训练及验证"><a href="#训练及验证" class="headerlink" title="训练及验证"></a>训练及验证</h3><p>训练阶段上文已叙述，主要讲一下验证时的操作。验证时输入为2张图片，分别来自gallery的HR图片和probe的LR图片。对于HR图片，直接经过Re-ID子网提取特征；对于LR图片，需要先进行SR子网的增强再经过Re-ID提取特征，最后度量L2距离。</p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>参考以下四张表格：<br><img src="https://s3.bmp.ovh/imgs/2022/09/05/da5efeb01914b978.png" alt="和现行方法的比较"><br><img src="https://s3.bmp.ovh/imgs/2022/09/05/3d02d144e35fddb2.png" alt="组合网络的比较"><br><img src="https://s3.bmp.ovh/imgs/2022/09/05/26de61efb1180638.png" alt="加入自制LR的效果"><br><img src="https://s3.bmp.ovh/imgs/2022/09/05/5bb3aae90e39a19b.png" alt="加入多尺度的效果"></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/07/14/论文笔记（一）/">论文笔记（一）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-07-14</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/论文笔记/">论文笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/行人重识别/">行人重识别</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/深度学习/">深度学习</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/图像处理/">图像处理</a></span><div class="content"><script src="/assets/js/APlayer.min.js"> </script><p><strong>CVPR2017</strong></p>
<h1 id="Re-ranking-Person-Re-identification-with-k-reciprocal-Encoding"><a href="#Re-ranking-Person-Re-identification-with-k-reciprocal-Encoding" class="headerlink" title="Re-ranking Person Re-identification with k-reciprocal Encoding"></a><strong>Re-ranking Person Re-identification with k-reciprocal Encoding</strong></h1><p><img src="https://s1.ax1x.com/2022/09/05/vTREUP.png" alt></p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>当将person re-ID看作一个检索过程时，re-ranking是提高其准确性的关键步骤。然而，在re-ID社区中，对re-ranking的努力有限，尤其是那些全自动、无监督的解决方案。在本文中，我们提出了一种k-reciprocal编码方法来re-ranking re-ID的结果。我们的假设是，如果一个gallery图像与k-reciprocal nearest neighbors中的probe查询相似，则更有可能是真正的匹配。具体地，给定图像，通过将其k-reciprocal nearest neighbors编码为单个向量来计算k-reciprocal特征，该向量用于在杰卡德距离（Jaccard Distance：用来衡量两个集合差异性的一种指标）下re-ranking。最终的距离计算为原始距离和杰卡德距离的组合。我们的re-ranking方法不需要任何人工交互或任何标记数据，因此适用于大规模数据集。在大型Market-1501、CUHK03、MARS和PRW数据集上的实验证实了我们方法的有效性。</p>
<hr>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>person re-ID是计算机视觉中的一个具有挑战性的课题。一般来说，re-ID可以被看作是一个检索问题。给定一个probe person，我们希望在gallery中搜索包含处于跨相机模式下的相同行人的图像。在获得初始排序列表之后，好的实践包括添加re-ranking步骤，期望相关图像将获得更高的排名。因此，本文将重点放在re-ranking问题上。</p>
<p>re-ranking主要是在通用实例检索generic instance retrieval[5、14、34、35]中进行研究。许多re-ranking主方法的主要优点是它可以在不需要额外训练样本的情况下实现，并且可以应用于任何初始ranking结果。</p>
<p>re-ranking的有效性在很大程度上取决于初始ranking列表initial ranking list的质量。许多先前的工作利用了初始排序列表[5，14，34，35，43，44]中排名靠前的图像(例如k-最近邻，k-nearest neighbors)之间的相似关系。一个基本的假设是，如果返回的图像在probe的k个最近邻内排序，那么它很可能是一个真正的匹配，可用于随后的re-ranking。然而，情况可能偏离最佳情况：错误匹配很可能包括在probe的k个最近邻中。例如，在图1中，P1、P2、P3和P4是4个查询图probe的真实匹配，但是它们都不包括在top-4中。我们观察到一些错误匹配（N1-N6）获得高排名。结果，直接使用top-k的图像可能在re-ranking系统中引入噪声并损害最终结果。<br><img src="https://s1.ax1x.com/2022/09/05/vTReC8.png" alt><br>在文献中，k-reciprocal nearest neighbor[14，34]是解决上述问题的有效方法，即被错误匹配污染的top-k图像。当两个图像被称为k-reciprocal nearest neighbor时，当另一个图像作为probe时，它们都被排到top-k。因此，k-reciprocal nearest neighbor作为两个图像是否正确匹配的更严格规则。在图1中，我们观察到probe是正确匹配图像的reciprocal neighbor，而不是错误匹配图像的reciprocal neighbor。该观察识别初始排序列表initial ranking list中的正确匹配，以改善重新排序re-ranking结果。</p>
<p>基于以上考虑，本文提出了一种基于k-reciprocal编码的re-ID re-ranking方法。我们的方法包括三个步骤。首先，将加权的k-reciprocal neighbor 集编码为一个向量，形成k-reciprocal特征。然后，两个图像之间的Jaccard距离可以通过它们的k-reciprocal特征来计算。其次，为了获得更鲁棒的k-reciprocal特征，我们改进了一种局部查询扩展方法（a local query expansion approach），以进一步改善re-ID性能。最后，最终距离的计算为原始距离和Jaccard距离的加权集合。随后，它被用来获取re-ranking列表。所提出的方法的框架如图2所示。综上所述，本文的贡献是：</p>
<ol>
<li>我们提出了一个k-reciprocal特征通过编码k-reciprocal特征到一个单一的向量。重re-ranking过程可以很容易地通过向量比较来执行。</li>
<li>我们的方法不需要任何人工交互或带标注的数据，并且可以自动和无监督的方式应用于任何人person re-ID ranking结果。</li>
<li>该方法有效地提高了Market-1501、CUHK03、MARS和PRW等数据集上的person re-ID性能。特别地，我们在rank-1和mAP上实现了Market-1501的最先进的精度。<br><img src="https://s1.ax1x.com/2022/09/05/vTRV4f.png" alt></li>
</ol>
<h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><p>我们推荐感兴趣的读者阅读[3，50]以详细回顾person re-ID。在此，我们重点研究用于目标检索，特别是用于re-ID的re-ranking方法。</p>
<p><strong>Re-ranking for object retrieval.</strong><br>Re-ranking方法已被成功地研究以提高目标检索精度[51]。许多工作利用k-nearest neighbors来探索相似关系来解决re-ranking问题。[5]提出了average query expansion (AQE)方法，该方法通过对top-k返回结果中的向量进行平均，得到一个新的查询向量（query vector），用于对数据库进行重新查询。为了利用远离查询图像的负样本，Arandjelović和Zisserman[1]改进了discriminative query expansion (DQE)，使用线性SVM来获得权重向量。从决策边界的距离被用来修改初始排序表（initial ranking list）。[35]利用初始排序表的k-nearest neighbors作为新查询（queries）来生成新的排序表。每个图像的新得分根据其在产生的排序表中的位置来计算。最近，稀疏上下文激活sparse contextual activation (SCA)[2]提出将neighbor set编码为向量，并通过广义Jaccard距离来表示样本的相似性。为了防止错误匹配对top-k图像的污染，[14，34]中采用了k-reciprocal nearest neighbors的概念。在[14]中提出了上下文不相似性度量contextual dissimilarity measure (CDM)，通过迭代正则化每个点到其邻域的平均距离来细化相似性。[34]正式提出k-reciprocal nearest neighbors的概念。k-reciprocal nearest neighbors被认为是高度相关的候选，用于构造闭集（closed set）以re-ranking数据集的其余部分。我们的工作从两个方面背离了这两个方法。我们不像文献[14]那样对最近邻（nearest neighborhood）关系进行对称化来细化相似度，也不像文献[34]那样直接将k-reciprocal nearest neighbors看作高阶样本。相反，我们通过比较两幅图像的k-reciprocal nearest neighbors来计算它们之间的新距离。</p>
<p><strong>Re-ranking for re-ID.</strong><br>大多数现有的person re-ID方法主要集中于特征表示[41，12，23，48，21]或度量学习[23，17，9，32，45]。最近，一些研究者[10，33，28，24，49，20，11，19，42，44]已经注意到在re-ID社区中基于re-ranking的方法。[20]通过分析每对图像的近邻（near neighbors）的相关信息和直接信息，建立re-ranking模型。在[11]中，通过联合考虑排序列表中的内容和上下文信息，学习无监督的重新排序模型，有效地去除了模糊样本，提高了re-ID的性能。[19]提出一种双向排序（bidirectional ranking）方法，利用计算得到的新相似度作为内容相似度和上下文相似度的融合，对初始排序表进行修正。最近，利用不同基线（different baseline）方法的公共最近邻来re-ranking任务[42，44]。[42]将全局特征和局部特征的公共最近邻作为新查询（queries），通过集合全局特征和局部特征的新排序列表来修改初始排序列表。在[44]中，利用k-nearest neighbor set从不同的baseline方法计算相似度和不相似度，然后进行相似度和不相似度的集合来优化初始排序表。上述方法在re-ranking方面继续取得进展，有望为将来从k-nearest neighbors发现进一步的信息作出贡献。然而，使用k-nearest neighbors直接实现re-ranking可能限制整体性能，因为常常包括错误匹配。<strong>为了解决这个问题，本文研究了k-reciprocal neighbors在person re-ID中的重要性，从而设计了一个简单而有效的re-ranking方法。</strong></p>
<h1 id="Proposed-Approach"><a href="#Proposed-Approach" class="headerlink" title="Proposed Approach"></a>Proposed Approach</h1><h3 id="Problem-Definition"><a href="#Problem-Definition" class="headerlink" title="Problem Definition"></a>Problem Definition</h3><p>给定查询图像<code>p</code>和<code>gallery set</code>（包含N幅图像，<code>G = {gi | i = 1, 2, ...N }</code>），<code>p</code>和<code>gi</code>之间的原始距离可以用马氏距离（Mahalanobis distance）衡量：<br><img src="https://s1.ax1x.com/2022/09/05/vTR9jH.png" alt><br>其中，<code>xp</code>个<code>xg</code>分别代表查询图<code>p</code>和<strong>检测集gallery</strong>中<code>gi</code>的外观特征，<code>M</code>是半正定矩阵。<br>初始排序表：<br><img src="https://s1.ax1x.com/2022/09/05/vTRpge.png" alt><br>可根据<code>probe p</code>和<code>gallery gi</code>之间的成对原始距离得到，其中：<br><img src="https://s1.ax1x.com/2022/09/05/vTRPud.png" alt><br>我们的目标是对<code>L(p,G)</code>进行<strong>re-rank</strong>，使更多的正样本排在<code>top</code>列表中，从而提高person re-ID的性能。</p>
<h3 id="K-reciprocal-Nearest-Neighbors"><a href="#K-reciprocal-Nearest-Neighbors" class="headerlink" title="K -reciprocal Nearest Neighbors"></a>K -reciprocal Nearest Neighbors</h3><p>我们将<code>N(p,k)</code>定义为一个probe p的k-nearest neighbors（i.e. 排序列表的top-k samples）：<br><img src="https://s1.ax1x.com/2022/09/05/vTRFHI.png" alt><br>其中，<code>|.|</code>表示集合中候选的数目。k-reciprocal nearest neighbors <code>R(p, k)</code>可以定义为：<br><img src="https://s1.ax1x.com/2022/09/05/vTRAEt.png" alt><br><strong>根据前面的描述，k-reciprocal nearest neighbors比k-nearest neighbors和probe p更相关</strong>。然而，由于照明、姿态、视图和遮挡的变化，正样本图像可能被从k-nearest neighbors中排除，并且随后不被包括在k-reciprocal nearest neighbors中。为了解决这个问题，我们根据以下条件将<code>R(p，k)</code>中每个候选项的<code>1/2 k-reciprocal nearest neighbors</code>增量地添加到更鲁棒的集合<code>R*(p，k)</code>中：<br><img src="https://s1.ax1x.com/2022/09/05/vTRiDA.png" alt></p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>在本文中，我们解决person re-ID的re-ranking问题。通过将k-reciprocal nearest neighbors编码为单个向量，我们提出了k-reciprocal特征，从而可以通过向量比较容易地执行re-ranking过程。为了从相似样本中获取相似关系，提出了局部扩展查询（local expansion query）以获得更鲁棒的k-reciprocal特征。基于原始距离和Jaccard距离的组合的最终距离有效地提高了多个大规模数据集上的re-ID性能。值得一提的是，我们的方法是全自动和无监督的，并且可以很容易地实现任何ranking结果。</p>
</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/56/"><i class="fa fa-chevron-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/56/">56</a><span class="page-number current">57</span><a class="page-number" href="/page/58/">58</a><span class="space">&hellip;</span><a class="page-number" href="/page/76/">76</a><a class="extend next" rel="next" href="/page/58/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://pic.syst.eu.org/WechatIMG8673.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2015 - 2023 By GeYu</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Enjoy the cyber world!</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>